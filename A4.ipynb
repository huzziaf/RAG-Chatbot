{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-10T18:48:59.013952Z","iopub.status.busy":"2024-05-10T18:48:59.012719Z","iopub.status.idle":"2024-05-10T18:48:59.456437Z","shell.execute_reply":"2024-05-10T18:48:59.455430Z","shell.execute_reply.started":"2024-05-10T18:48:59.013899Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\HP\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from qdrant_client import models, QdrantClient\n","from sentence_transformers import SentenceTransformer\n","import pandas as pd\n","from transformers import pipeline\n","from flask import Flask, render_template, request"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:47:25.610634Z","iopub.status.busy":"2024-05-10T18:47:25.610055Z","iopub.status.idle":"2024-05-10T18:47:31.648640Z","shell.execute_reply":"2024-05-10T18:47:31.647432Z","shell.execute_reply.started":"2024-05-10T18:47:25.610600Z"},"trusted":true},"outputs":[],"source":["#used for sentence embeddings for vector DB storage\n","encoder = SentenceTransformer('all-MiniLM-L6-v2') "]},{"cell_type":"markdown","metadata":{},"source":["## Setting up vector database"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:47:41.110611Z","iopub.status.busy":"2024-05-10T18:47:41.107286Z","iopub.status.idle":"2024-05-10T18:47:41.115991Z","shell.execute_reply":"2024-05-10T18:47:41.114975Z","shell.execute_reply.started":"2024-05-10T18:47:41.110544Z"},"trusted":true},"outputs":[],"source":["qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:48:08.215155Z","iopub.status.busy":"2024-05-10T18:48:08.214688Z","iopub.status.idle":"2024-05-10T18:48:08.227509Z","shell.execute_reply":"2024-05-10T18:48:08.226332Z","shell.execute_reply.started":"2024-05-10T18:48:08.215122Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\2414972111.py:2: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n","  qdrant.recreate_collection(\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Create collection to store books\n","qdrant.recreate_collection(\n","    collection_name=\"thesis-info\",\n","    vectors_config=models.VectorParams(\n","        size=encoder.get_sentence_embedding_dimension(), \n","        distance=models.Distance.COSINE\n","    )\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Reading and storing data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:49:47.389323Z","iopub.status.busy":"2024-05-10T18:49:47.388364Z","iopub.status.idle":"2024-05-10T18:49:47.396628Z","shell.execute_reply":"2024-05-10T18:49:47.395805Z","shell.execute_reply.started":"2024-05-10T18:49:47.389277Z"},"trusted":true},"outputs":[],"source":["file_path = 'dataset.xlsx'\n","data_list = []"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T18:50:27.328885Z","iopub.status.busy":"2024-05-10T18:50:27.328236Z","iopub.status.idle":"2024-05-10T18:50:30.259708Z","shell.execute_reply":"2024-05-10T18:50:30.258529Z","shell.execute_reply.started":"2024-05-10T18:50:27.328853Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n","c:\\Users\\HP\\anaconda3\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1050692152.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  'abstract': row[3]\n"]}],"source":["with pd.ExcelFile(file_path) as xls:\n","        for sheet_name in xls.sheet_names:\n","            df = pd.read_excel(xls, sheet_name, skiprows=1)\n","            for index, row in df.iterrows():\n","                documents = {\n","                    'title': ''.join([word for word in row[1].split('\\n')[1:] if word]),\n","                    'abstract': row[3]\n","                }\n","                data_list.append(documents)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T19:07:58.834337Z","iopub.status.busy":"2024-05-10T19:07:58.832369Z","iopub.status.idle":"2024-05-10T19:09:29.546909Z","shell.execute_reply":"2024-05-10T19:09:29.545576Z","shell.execute_reply.started":"2024-05-10T19:07:58.834271Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17044\\1671314003.py:1: DeprecationWarning: `upload_records` is deprecated, use `upload_points` instead\n","  qdrant.upload_records(\n"]}],"source":["qdrant.upload_records(\n","    collection_name=\"thesis-info\",\n","    records=[\n","        models.Record(\n","            id=idx,\n","            vector=encoder.encode(doc[\"abstract\"]).tolist(),\n","            payload=doc\n","        ) for idx, doc in enumerate(data_list)\n","    ]\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T19:11:52.051873Z","iopub.status.busy":"2024-05-10T19:11:52.051501Z","iopub.status.idle":"2024-05-10T19:11:52.115691Z","shell.execute_reply":"2024-05-10T19:11:52.113638Z","shell.execute_reply.started":"2024-05-10T19:11:52.051842Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'title': 'Muhammad Umair Arshad (MS-CS)Supervisor: Dr. Waseem Shahzad', 'abstract': 'Typically, there are words in all languages which may have multiple senses/meanings. The meaning/sense associated with a word is referred to as the Word Sense. In ad-dition, ascertaining the correct sense of such a word with reference to its context and use is called Word Sense Disambiguation (WSD). This research focuses on WSD for the Urdu language. Previously, a lot of work on WSD has been performed for the English language, but WSD for Urdu is still in infancy due to rich morphological structure of the language. Therefore, there is a need to explore whether the techniques used for other languages work well on the Urdu language. Moreover, a number of initiatives have resulted in the development of the WSD corpora benchmark for a wide range of languages from different language families. Nevertheless, there is an absence of bench-mark WSD corpora for South Asian languages including Urdu, despite more than 300 million Urdu speakers and a lot of digital Urdu text online. The importance of this exploration is amplified by the fact that WSD is very crucial for some core problems of NLP like information retrieval, machine translation, speech processing, question answering and, text categorization tasks etc. Previous work uses machine learning algorithms like Naive Bayes, decision trees, SVM etc. In this thesis, we explore the use of Long short-term memory (LSTM) for WSD for Urdu language. It is a supervised learning algorithm for WSD on Urdu dataset. We applied different baseline algorithms like SVM, Decision tree, Naive Bayes and Random Forest on our dataset. For testing, we used k-fold validation. Extensive experiments have been performed to evaluate our approach. The results showed a better F-measure than SVM, Decision tree, Naive Bayes and Random Forest. For the proposed approach, the maximum F-measure was recorded over two hundred and seventy-one million words raw Urdu corpus.'} score: 0.3557601112734944\n","{'title': 'Automatic Multilingual Detection for Local Languages', 'abstract': 'Social media shows the rapid growth of modernity; with time, the amount of data increases over the Internet. The problem of language detection is one of the current challenges in Natural Language Processing (NLP). The goal of similar language detection is to correctly and accurately detect the language of text written. It plays a vital role in several Natural Language Processing (NLP) applications; it is frequently used as preprocessing technique. Multilingual detection is detecting a natural language of a text based on the sentence’s content. Multilingual detection is challenging when the input is noisy data, low resource languages, and languages are highly similar. Similar language detection remains a challenging  task in NLP. In this research work, we discuss sentence collection in a corpus of Pashto, Punjabi, Saraiki, and Urdu languages. We proposed a novel dataset that contains sentences of low resource languages; Our dataset contains 21000 sentences. The corpus of the four languages compiles from newspaper websites. First of all, we performed similar language detection without using machine learning techniques; we detected language by the dictionary-based method. Four dictionaries are extracted for each language using the training dataset. Our dictionaries-based process does not perform effectively because of limited words in dictionaries. Secondly, we extracted Features by TF-IDF vectorizer. To detect the sentence’s language, We use five different classifiers: Decision Tree, K-Nearest Neighbours (KNN), Logistic Regression, Neural Network, Naive Bayes, and Random Forest. Few of the classifiers performed well. Thirdly we extracted word embeddings by the Word2Vec technique, and to detect the language of the sentence, we used machine learning classifiers few of them performed well. Fourthly In our proposed approach, we proposed a deep\\nlearning model with Bidirectional Encoder Representation from Transformers (BERT); we pre-train our multilingual BERT for similar multilingual detection. BERT takes advantage of the mechanism of self-attention to integrate contextual information. We show through our experiments that our proposed approach produces notable performance improvement. Keywords: Automatic Multilingual Detection, Local Languages, Word Embeddings, Highly Similar, Transformers, Classifiers.'} score: 0.3240865161807358\n","{'title': 'Muhammad Owais Idrees (MS-CS)Supervisor: Dr. Hasan Mujtaba  ', 'abstract': \"The free speech on social media enables people to express their emotions and ideas on social media or verbally. On the other hand, that speech can hurt someones feeling and can be a case of mental torture. Sonmething that is ottensive upsets or enmbarrasses people because it is rude or insulting is called offensive language. Nowadays, the use of social media has increased to a great extent and it has become a source of expression for people's feelings and reviews. Due to free speech, people are always afraid of being attacked or criticized by someone. Although some social media platfornm provides the option of filtering the content that is limited to blacklist word. So, there is a requirement for automatic detection of offense language online and take some action against the predator/ extremist. We propose a model for the detection of oftensive language in the Urdu language social media. In literature, it has been reported that most of the researchers used the blacklist technicque and other features to predict the offensive language. Our work explores the context and subject of the sentence that have much impact on the sentence. So, we take the context and subject as the main features and use deep neural networks for the classification of offensive language. We explore more features including the polarity of the sentence and train\\nmodel on deep neural networks with attention and LSTMs layers because it maintains the context in a good manner, which can be seen through its success in sequencing tasks. The model is compared with its benchmark by using the Fl evaluation matrix and achieve good results. This approach was challenging because of less availability of data. We create a new dataset tor our research purpose. We will make our modeleasily available for the public and future research \"} score: 0.31056355938051416\n"]}],"source":["hits = qdrant.search(\n","    collection_name=\"thesis-info\",\n","    query_vector=encoder.encode(\"language\").tolist(),\n","    limit=3\n",")\n","for hit in hits:\n","  print(hit.payload, \"score:\", hit.score)"]},{"cell_type":"markdown","metadata":{},"source":["## Integrating LLM and testing prompt generation"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-10T19:21:09.936490Z","iopub.status.busy":"2024-05-10T19:21:09.936063Z","iopub.status.idle":"2024-05-10T19:21:18.512847Z","shell.execute_reply":"2024-05-10T19:21:18.511643Z","shell.execute_reply.started":"2024-05-10T19:21:09.936455Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["generator = pipeline(\"text-generation\", model=\"gpt2\")\n","\n","def query_vector_database(user_input):\n","    hits = qdrant.search(\n","    collection_name=\"thesis-info\",\n","    query_vector=encoder.encode(user_input).tolist(),\n","    limit=1\n",")\n","    for hit in hits:\n","      return hit.payload\n","\n","def generate_response(user_input, relevant_theses):\n","    prompt = f\"User input: {user_input}\\n Relevant theses: {relevant_theses}\\n\"\n","    generated_text = generator(prompt, max_length=500, num_return_sequences=1, truncation=True)[0]['generated_text']\n","    return generated_text.strip()\n","\n","user_input = \"what do you know about Prosodic alignment for Automatic dubbing\"\n","\n","relevant_theses = query_vector_database(user_input)\n","response = generate_response(user_input, relevant_theses)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User input: what do you know about Prosodic alignment for Automatic dubbing\n"," Relevant theses: {'title': 'Prosodic alignment for Automatic dubbing', 'abstract': 'Automatic dubbing is the process of replacing the audio track of a video with a different language. In automatic dubbing, prosodic alignment is used to match the suprasegmental features like timbre, prosody, duration, pauses and intonation of the original speech with synthesed speech, in order to produce a natural-sounding dubbed video. This is done by analyzing and mapping these features of the original and translated speech. Existing research on automatic dubbing lack to addresses these features in source video which impact the overall naturalness and fluency of Synthesized speech. To solve this we proposed end-to-end architecture, following modular approach, to generate high quality dubbed video. In this research, we mainly focus on TTS module by considering mentioned features for prosodic alignment in generated speech with synchronization of original utterance. We train our model learn to predict the suprasegmental features of the input voice and generate synthesized speech that matches the pattern of stress and intonation of the input voice. This help our proposed model to create a more natural and coherent output and improve the overall accuracy of end to end architecture. We explore auto-regressive decoder models and Constructive voice-voice pre-training model for our TTS module. We used a common voice English dataset which contains English text and corresponding voices. To evaluate our model we perform Quantitative Evaluation by calculating Mel Cepstral Distortion (MCD) with Dynamic Time Warping (DTW) and Word Error Rate.'}\n","\n","Topics: Audio, speech, transcoding, tessellations, speech perception. | [http://www.youtube.com/watch?v=3YX8ZKzYy7Fk]\n","\n","Topics: Speech, Transliteration, Auditory Auditioning, Acoustic Auditioning, Vocabulary | [https://s3.amazonaws.com/S3-Voice-Fiction/Audible-Voicing-TTS.htm] | Categories: Video, Audio, Vocabulary, Sound, Digital Media | [https://s3.amazonaws.com/S3-Voice-Fiction/Unfinished-Vocabulary.htm] | Category: Audio, Speech,\n"]}],"source":["print(response)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4982150,"sourceId":8378482,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
